{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ee0309",
   "metadata": {},
   "source": [
    "### original test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a9ed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xnli (/home/s6amalia/.cache/huggingface/datasets/xnli/default-language=es/1.1.0/818164464f9c9fd15776ca8a00423b074344c3e929d00a2c1a84aa5a50c928bd)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bien, ni estaba pensando en eso, pero estaba t...</td>\n",
       "      <td>No he vuelto a hablar con él.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bien, ni estaba pensando en eso, pero estaba t...</td>\n",
       "      <td>Estaba tan molesto que empecé a hablar con él ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bien, ni estaba pensando en eso, pero estaba t...</td>\n",
       "      <td>Tuvimos una gran charla.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y pensé que era un privilegio, y todavía es, t...</td>\n",
       "      <td>No sabía que no era la única persona en el cam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y pensé que era un privilegio, y todavía es, t...</td>\n",
       "      <td>Tenía la impresión de que era el único con ese...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5005</th>\n",
       "      <td>Davidson no debería adoptar la pronunciación d...</td>\n",
       "      <td>Davidson no debería hablar de manera que bone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5006</th>\n",
       "      <td>Davidson no debería adoptar la pronunciación d...</td>\n",
       "      <td>Sería mejor si Davidson rimara las palabras bo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5007</th>\n",
       "      <td>La novela media de 200 000 palabras por 25 $ f...</td>\n",
       "      <td>Una novela de 200.000 palabras a 25 dólares es...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5008</th>\n",
       "      <td>La novela media de 200 000 palabras por 25 $ f...</td>\n",
       "      <td>Una novela de 200 000 palabras de 25 $ son 400...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5009</th>\n",
       "      <td>La novela media de 200 000 palabras por 25 $ f...</td>\n",
       "      <td>Una novela de 200 000 palabras de 25 $ son 800...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5010 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                premise  \\\n",
       "0     Bien, ni estaba pensando en eso, pero estaba t...   \n",
       "1     Bien, ni estaba pensando en eso, pero estaba t...   \n",
       "2     Bien, ni estaba pensando en eso, pero estaba t...   \n",
       "3     Y pensé que era un privilegio, y todavía es, t...   \n",
       "4     Y pensé que era un privilegio, y todavía es, t...   \n",
       "...                                                 ...   \n",
       "5005  Davidson no debería adoptar la pronunciación d...   \n",
       "5006  Davidson no debería adoptar la pronunciación d...   \n",
       "5007  La novela media de 200 000 palabras por 25 $ f...   \n",
       "5008  La novela media de 200 000 palabras por 25 $ f...   \n",
       "5009  La novela media de 200 000 palabras por 25 $ f...   \n",
       "\n",
       "                                             hypothesis  label  \n",
       "0                         No he vuelto a hablar con él.      2  \n",
       "1     Estaba tan molesto que empecé a hablar con él ...      0  \n",
       "2                              Tuvimos una gran charla.      1  \n",
       "3     No sabía que no era la única persona en el cam...      1  \n",
       "4     Tenía la impresión de que era el único con ese...      0  \n",
       "...                                                 ...    ...  \n",
       "5005  Davidson no debería hablar de manera que bone ...      0  \n",
       "5006  Sería mejor si Davidson rimara las palabras bo...      2  \n",
       "5007  Una novela de 200.000 palabras a 25 dólares es...      1  \n",
       "5008  Una novela de 200 000 palabras de 25 $ son 400...      2  \n",
       "5009  Una novela de 200 000 palabras de 25 $ son 800...      0  \n",
       "\n",
       "[5010 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "language = 'es'\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('xnli', split='test',language=language)\n",
    "df_test = pd.DataFrame(dataset)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050c06b",
   "metadata": {},
   "source": [
    "### Test data with typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0058f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Y pensé que era un privilegio, y todavía es, ...</td>\n",
       "      <td>'A todos nos dieron el mismo número exacto, si...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'así que no tengo muy claro por qué.'</td>\n",
       "      <td>'Estoy seguro de la razon por la cuál.'</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'El chica que me puede ayudar está en la otra ...</td>\n",
       "      <td>'La chica de al que necesito ayuda vive muy le...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'pero ellos estaban divididos acerca de quiéne...</td>\n",
       "      <td>'No podían ponerse de acuerdo sobe quién era u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'pero ellos estaban divididos acerca de quiene...</td>\n",
       "      <td>'No podían estar de acuerdo con quién deberia ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>'Uno debe ser muy cauto a la hora de proponer ...</td>\n",
       "      <td>'es delicado introducir las nuevas etimologías.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>'Davidson no debería adoptar la pronunciación ...</td>\n",
       "      <td>'Davidson no debería hablar de manera que bone...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>'Davidson no debería adoptar la pronunciación ...</td>\n",
       "      <td>'Sería Mejor si Davidson rimara las palabras b...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>'La novela media de 200 000 palabras por 25 $f...</td>\n",
       "      <td>'Una novela de 200 000 palabras de 25 $son 400...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>'La novela media de 200 000 palabras por 25 $f...</td>\n",
       "      <td>'Una novela de 200 000 palabras de 25 $son 800...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>687 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               premise  \\\n",
       "0    'Y pensé que era un privilegio, y todavía es, ...   \n",
       "1                'así que no tengo muy claro por qué.'   \n",
       "2    'El chica que me puede ayudar está en la otra ...   \n",
       "3    'pero ellos estaban divididos acerca de quiéne...   \n",
       "4    'pero ellos estaban divididos acerca de quiene...   \n",
       "..                                                 ...   \n",
       "682  'Uno debe ser muy cauto a la hora de proponer ...   \n",
       "683  'Davidson no debería adoptar la pronunciación ...   \n",
       "684  'Davidson no debería adoptar la pronunciación ...   \n",
       "685  'La novela media de 200 000 palabras por 25 $f...   \n",
       "686  'La novela media de 200 000 palabras por 25 $f...   \n",
       "\n",
       "                                            hypothesis  label  \n",
       "0    'A todos nos dieron el mismo número exacto, si...      2  \n",
       "1              'Estoy seguro de la razon por la cuál.'      2  \n",
       "2    'La chica de al que necesito ayuda vive muy le...      0  \n",
       "3    'No podían ponerse de acuerdo sobe quién era u...      0  \n",
       "4    'No podían estar de acuerdo con quién deberia ...      1  \n",
       "..                                                 ...    ...  \n",
       "682   'es delicado introducir las nuevas etimologías.'      1  \n",
       "683  'Davidson no debería hablar de manera que bone...      0  \n",
       "684  'Sería Mejor si Davidson rimara las palabras b...      2  \n",
       "685  'Una novela de 200 000 palabras de 25 $son 400...      2  \n",
       "686  'Una novela de 200 000 palabras de 25 $son 800...      0  \n",
       "\n",
       "[687 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = r\"/home/s6amalia/thesis/XNLI_noisy_dataset/\"\n",
    "df_typo = pd.read_table(DATA_DIR+'test_'+language+'_typos_0.05.tsv')\n",
    "df_typo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6b499",
   "metadata": {},
   "source": [
    "### Find original test data corresponding to the typo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebb2a9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 682 of 687\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix1 = vectorizer.fit_transform(df_typo['hypothesis'])\n",
    "tfidf_matrix2 = vectorizer.transform(df_test['hypothesis'])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix1, tfidf_matrix2)\n",
    "\n",
    "# Threshold for considering texts as similar; change it to find the best number of data\n",
    "threshold = 0.81\n",
    "\n",
    "# Find similar rows\n",
    "similar_rows = []\n",
    "for i in range(len(df_typo)):\n",
    "    for j in range(len(df_test)):\n",
    "        if cosine_sim[i, j] > threshold:\n",
    "            similar_rows.append((i, j))\n",
    "columns = df_test.columns\n",
    "similar_df = []\n",
    "\n",
    "for pair in similar_rows:\n",
    "    similar_df.append(df_test.iloc[pair[1]])\n",
    "df_test_1 = pd.DataFrame(similar_df)   \n",
    "\n",
    "print('Found',len(df_test_1),'of', len(df_typo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2208a7d9",
   "metadata": {},
   "source": [
    "### Number of typos in each sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c88ca587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "def typo_count(s1,s2):\n",
    "    \n",
    "    # Create a Differ object\n",
    "    differ = difflib.Differ()\n",
    "\n",
    "    # Compare the two texts line by line\n",
    "    diff = list(differ.compare(s1.splitlines(), s2.splitlines()))\n",
    "    num_changes = sum(1 for line in diff if line.startswith('-') or line.startswith('+'))\n",
    "    return num_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1def8815",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m typos \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df_test)):\n\u001b[0;32m----> 3\u001b[0m     s2 \u001b[38;5;241m=\u001b[39m df_typo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpremise\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m     s1 \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpremise\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[1;32m      5\u001b[0m     typos\u001b[38;5;241m.\u001b[39mappend(typo_count(s1,s2))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1656\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1589\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1587\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1589\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "typos = []\n",
    "for i in range(len(df_test)):\n",
    "    s2 = df_typo['premise'].iloc[i][1:-1]\n",
    "    s1 = df_test['premise'].iloc[i]\n",
    "    typos.append(typo_count(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96cf6488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typo_count(df_test['premise'].iloc[100],df_typo['premise'].iloc[100][1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346f442",
   "metadata": {},
   "source": [
    "### Predict labels and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a8609e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ca93e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,XLMRobertaForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/s6amalia/xlmroberta-xnli-\"+language+\".tk\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "## loading the fine-tuned model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"/home/s6amalia/xlmroberta-xnli-\"+language+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23d9a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(df):\n",
    "    all_pred = []\n",
    "    for i in range(len(df)):\n",
    "        premise = df['premise'].iloc[i]\n",
    "        hypothesis = df['hypothesis'].iloc[i]\n",
    "        input = tokenizer(premise, hypothesis, truncation=True,padding=True, return_tensors=\"pt\")\n",
    "        output = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "#         print(output)\n",
    "        prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "        label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "        # prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "        all_pred.append(prediction.index(max(prediction)))\n",
    "        print(int(i*100/len(df)),'%', end='\\r')\n",
    "    print(round( accuracy_score(all_pred, df['label'])*100,2))   \n",
    "    return all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd375627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.02\n"
     ]
    }
   ],
   "source": [
    "all_pred = predict_labels(df_typo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "293dbd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.65\n"
     ]
    }
   ],
   "source": [
    "all_pred = predict_labels(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5cb89f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.47\n"
     ]
    }
   ],
   "source": [
    "all_pred = predict_labels(df_test_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
