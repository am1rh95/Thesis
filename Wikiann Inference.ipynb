{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ec981ab",
   "metadata": {},
   "source": [
    "### Loading original testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998769e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikiann (/home/s6amalia/.cache/huggingface/datasets/wikiann/de/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38e0acb97824ff49f8d425ec5c3be39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the WikiANN dataset\n",
    "dataset = load_dataset(\"wikiann\", \"de\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0077812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>langs</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WEITERLEITUNG Hu ( Xi’an )</td>\n",
       "      <td>[0, 5, 6, 6, 6]</td>\n",
       "      <td>[de, de, de, de, de]</td>\n",
       "      <td>[LOC: Hu ( Xi’an )]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Katja Kipping , Bernd Riexinger</td>\n",
       "      <td>[1, 2, 0, 1, 2]</td>\n",
       "      <td>[de, de, de, de, de]</td>\n",
       "      <td>[PER: Katja Kipping, PER: Bernd Riexinger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Runde rammte Lorenzo Bandini an zweiter Stelle...</td>\n",
       "      <td>[0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[de, de, de, de, de, de, de, de, de, de, de, d...</td>\n",
       "      <td>[PER: Lorenzo Bandini]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Im Doppel waren Marcelo Melo und André Sá die ...</td>\n",
       "      <td>[0, 0, 0, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[de, de, de, de, de, de, de, de, de, de, de, d...</td>\n",
       "      <td>[PER: Marcelo Melo, PER: André Sá]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>' '' Gary Glitter '' ' - ''Always Yours ''</td>\n",
       "      <td>[0, 0, 1, 2, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[de, de, de, de, de, de, de, de, de, de]</td>\n",
       "      <td>[PER: Gary Glitter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Die Art kommt in der südlich der Sahelzone gel...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 6, 0, 5, 0, 0, ...</td>\n",
       "      <td>[de, de, de, de, de, de, de, de, de, de, de, d...</td>\n",
       "      <td>[LOC: Sahelzone, LOC: westsudanischen Savanne,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Danach stand er zeitweilig in schwedischen Die...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 5, 0, 0]</td>\n",
       "      <td>[de, de, de, de, de, de, de, de]</td>\n",
       "      <td>[LOC: schwedischen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Des Weiteren schließt die seit Einstellung des...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, ...</td>\n",
       "      <td>[de, de, de, de, de, de, de, de, de, de, de, d...</td>\n",
       "      <td>[ORG: Scheduled Monument]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Aostatal , Piemont , Lombardei , Ligurien</td>\n",
       "      <td>[5, 0, 5, 0, 5, 0, 5]</td>\n",
       "      <td>[de, de, de, de, de, de, de]</td>\n",
       "      <td>[LOC: Aostatal, LOC: Piemont, LOC: Lombardei, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>des Abgeordnetenhauses von Berlin .</td>\n",
       "      <td>[0, 3, 4, 4, 0]</td>\n",
       "      <td>[de, de, de, de, de]</td>\n",
       "      <td>[ORG: Abgeordnetenhauses von Berlin]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tokens  \\\n",
       "0                            WEITERLEITUNG Hu ( Xi’an )   \n",
       "1                       Katja Kipping , Bernd Riexinger   \n",
       "2     Runde rammte Lorenzo Bandini an zweiter Stelle...   \n",
       "3     Im Doppel waren Marcelo Melo und André Sá die ...   \n",
       "4            ' '' Gary Glitter '' ' - ''Always Yours ''   \n",
       "...                                                 ...   \n",
       "9995  Die Art kommt in der südlich der Sahelzone gel...   \n",
       "9996  Danach stand er zeitweilig in schwedischen Die...   \n",
       "9997  Des Weiteren schließt die seit Einstellung des...   \n",
       "9998          Aostatal , Piemont , Lombardei , Ligurien   \n",
       "9999                des Abgeordnetenhauses von Berlin .   \n",
       "\n",
       "                                               ner_tags  \\\n",
       "0                                       [0, 5, 6, 6, 6]   \n",
       "1                                       [1, 2, 0, 1, 2]   \n",
       "2               [0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3     [0, 0, 0, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4                        [0, 0, 1, 2, 0, 0, 0, 0, 0, 0]   \n",
       "...                                                 ...   \n",
       "9995  [0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 6, 0, 5, 0, 0, ...   \n",
       "9996                           [0, 0, 0, 0, 0, 5, 0, 0]   \n",
       "9997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, ...   \n",
       "9998                              [5, 0, 5, 0, 5, 0, 5]   \n",
       "9999                                    [0, 3, 4, 4, 0]   \n",
       "\n",
       "                                                  langs  \\\n",
       "0                                  [de, de, de, de, de]   \n",
       "1                                  [de, de, de, de, de]   \n",
       "2     [de, de, de, de, de, de, de, de, de, de, de, d...   \n",
       "3     [de, de, de, de, de, de, de, de, de, de, de, d...   \n",
       "4              [de, de, de, de, de, de, de, de, de, de]   \n",
       "...                                                 ...   \n",
       "9995  [de, de, de, de, de, de, de, de, de, de, de, d...   \n",
       "9996                   [de, de, de, de, de, de, de, de]   \n",
       "9997  [de, de, de, de, de, de, de, de, de, de, de, d...   \n",
       "9998                       [de, de, de, de, de, de, de]   \n",
       "9999                               [de, de, de, de, de]   \n",
       "\n",
       "                                                  spans  \n",
       "0                                   [LOC: Hu ( Xi’an )]  \n",
       "1            [PER: Katja Kipping, PER: Bernd Riexinger]  \n",
       "2                                [PER: Lorenzo Bandini]  \n",
       "3                    [PER: Marcelo Melo, PER: André Sá]  \n",
       "4                                   [PER: Gary Glitter]  \n",
       "...                                                 ...  \n",
       "9995  [LOC: Sahelzone, LOC: westsudanischen Savanne,...  \n",
       "9996                                [LOC: schwedischen]  \n",
       "9997                          [ORG: Scheduled Monument]  \n",
       "9998  [LOC: Aostatal, LOC: Piemont, LOC: Lombardei, ...  \n",
       "9999               [ORG: Abgeordnetenhauses von Berlin]  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ds=pd.DataFrame(dataset['test'])\n",
    "def try_join(l):\n",
    "    return ' '.join(l)\n",
    "ds['tokens'] = [try_join(l) for l in ds['tokens']]\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86f380",
   "metadata": {},
   "source": [
    "### Loading noisy testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d9a336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/s6amalia/.cache/huggingface/datasets/json/default-fff621086d415b16/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5157c434c4e54ec78bc57670cedcc03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_typo =load_dataset(\"json\",data_files = '/home/s6amalia/thesis/Wikiann_noisy_dataset/test_de_typos_0.2.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02abea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>langs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WEITERLEITUNG Hu ( Xi'an )</td>\n",
       "      <td>[0, 5, 6, 6, 6]</td>\n",
       "      <td>[de, de, de, de, de]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Runde rammte Lorenzo Bandini an zweite Stelle ...</td>\n",
       "      <td>[0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[de, de, de, de, de, de, de, de, de, de, de, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Im Doppel war Marcelo Melo und André Sá die Ti...</td>\n",
       "      <td>[0, 0, 0, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[de, de, de, de, de, de, de, de, de, de, de, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexandrasittiche kommen außerdem in dem Galer...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[de, de, de, de, de, de, de, de, de, de, de]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Die Cover zeigt sie zusammen Mit Kate Moss un ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 2, 0]</td>\n",
       "      <td>[de, de, de, de, de, de, de, de, de, de, de, de]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7917</th>\n",
       "      <td>** '' die Sopranos '' , HBO</td>\n",
       "      <td>[0, 0, 3, 4, 0, 0, 3]</td>\n",
       "      <td>[de, de, de, de, de, de, de]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7918</th>\n",
       "      <td>Die Art kommt in dem südlich der Sahelzone gel...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 6, 0, 5, 0, 0, ...</td>\n",
       "      <td>[de, de, de, de, de, de, de, de, de, de, de, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7919</th>\n",
       "      <td>Danach standen er zeitweilig in Schwedischen D...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 5, 0, 0]</td>\n",
       "      <td>[de, de, de, de, de, de, de, de]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7920</th>\n",
       "      <td>des weiteren schließt der seit Einstellung des...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, ...</td>\n",
       "      <td>[de, de, de, de, de, de, de, de, de, de, de, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7921</th>\n",
       "      <td>der Abgeordnetenhauses von Berlin .</td>\n",
       "      <td>[0, 3, 4, 4, 0]</td>\n",
       "      <td>[de, de, de, de, de]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7922 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tokens  \\\n",
       "0                            WEITERLEITUNG Hu ( Xi'an )   \n",
       "1     Runde rammte Lorenzo Bandini an zweite Stelle ...   \n",
       "2     Im Doppel war Marcelo Melo und André Sá die Ti...   \n",
       "3     Alexandrasittiche kommen außerdem in dem Galer...   \n",
       "4     Die Cover zeigt sie zusammen Mit Kate Moss un ...   \n",
       "...                                                 ...   \n",
       "7917                        ** '' die Sopranos '' , HBO   \n",
       "7918  Die Art kommt in dem südlich der Sahelzone gel...   \n",
       "7919  Danach standen er zeitweilig in Schwedischen D...   \n",
       "7920  des weiteren schließt der seit Einstellung des...   \n",
       "7921                der Abgeordnetenhauses von Berlin .   \n",
       "\n",
       "                                               ner_tags  \\\n",
       "0                                       [0, 5, 6, 6, 6]   \n",
       "1               [0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2     [0, 0, 0, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3                     [0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0]   \n",
       "4                  [0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 2, 0]   \n",
       "...                                                 ...   \n",
       "7917                              [0, 0, 3, 4, 0, 0, 3]   \n",
       "7918  [0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 6, 0, 5, 0, 0, ...   \n",
       "7919                           [0, 0, 0, 0, 0, 5, 0, 0]   \n",
       "7920  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, ...   \n",
       "7921                                    [0, 3, 4, 4, 0]   \n",
       "\n",
       "                                                  langs  \n",
       "0                                  [de, de, de, de, de]  \n",
       "1     [de, de, de, de, de, de, de, de, de, de, de, d...  \n",
       "2     [de, de, de, de, de, de, de, de, de, de, de, d...  \n",
       "3          [de, de, de, de, de, de, de, de, de, de, de]  \n",
       "4      [de, de, de, de, de, de, de, de, de, de, de, de]  \n",
       "...                                                 ...  \n",
       "7917                       [de, de, de, de, de, de, de]  \n",
       "7918  [de, de, de, de, de, de, de, de, de, de, de, d...  \n",
       "7919                   [de, de, de, de, de, de, de, de]  \n",
       "7920  [de, de, de, de, de, de, de, de, de, de, de, d...  \n",
       "7921                               [de, de, de, de, de]  \n",
       "\n",
       "[7922 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_typo=pd.DataFrame(dataset_typo['train'])\n",
    "def try_join(l):\n",
    "    return ' '.join(l)\n",
    "df_typo['tokens'] = [try_join(l) for l in df_typo['tokens']]\n",
    "df_typo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d666644",
   "metadata": {},
   "source": [
    "### Find original test data corresponding to the noisy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9eed9831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7841 of 7922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix1 = vectorizer.fit_transform(df_typo['tokens'])\n",
    "tfidf_matrix2 = vectorizer.transform(ds['tokens'])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix1, tfidf_matrix2)\n",
    "\n",
    "# Threshold for considering texts as similar; change it to find the best number of data\n",
    "threshold = 0.72\n",
    "\n",
    "# Find similar rows\n",
    "similar_rows = []\n",
    "for i in range(len(df_typo)):\n",
    "    for j in range(len(ds)):\n",
    "        if cosine_sim[i, j] > threshold:\n",
    "            similar_rows.append((i, j))\n",
    "columns = ds.columns\n",
    "similar_df = []\n",
    "\n",
    "for pair in similar_rows:\n",
    "    similar_df.append(ds.iloc[pair[1]])\n",
    "df_test_1 = pd.DataFrame(similar_df)   \n",
    "\n",
    "print('Found',len(df_test_1),'of', len(df_typo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b70a6",
   "metadata": {},
   "source": [
    "### Loading pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f81bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification\n",
    "import pandas as pd\n",
    "import torch\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"/home/s6amalia/xlmroberta-wikiann-de.pt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/s6amalia/xlmroberta-wikiann-de.tk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd516fc1",
   "metadata": {},
   "source": [
    "### Predict labels and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c076bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(dataset):\n",
    "    right_count = 0\n",
    "    wrong_count = 0\n",
    "    for i in range(len(dataset)):\n",
    "        print(int(i*100/len(dataset)),'%', end='\\r')\n",
    "        inputs = dataset['tokens'][i]\n",
    "        inputs = tokenizer(inputs,truncation=True, is_split_into_words=True,return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "#         predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
    "        res=pd.DataFrame([inputs.word_ids(batch_index=0),predictions[0].tolist()])\n",
    "\n",
    "    # tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        res=res.transpose()\n",
    "        res.columns=['idx','label']\n",
    "        res=res.drop_duplicates(subset=['idx'], keep='first')[1:]\n",
    "        res['true_label']  = dataset[i]['ner_tags']\n",
    "        r = 0\n",
    "        w = 0\n",
    "        for j in range(len(res)):\n",
    "            if (res['true_label'].iloc[j] == res['label'].iloc[j]):\n",
    "                if res['true_label'].iloc[j] != 0 :\n",
    "                    r = r+1\n",
    "            else:         \n",
    "                w = w +1 \n",
    "\n",
    "        right_count = right_count + r\n",
    "        wrong_count = wrong_count + w\n",
    "        \n",
    "    return right_count,wrong_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd73097",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Noisy Dataset\n",
    "right_count , wrong_count = cal_accuracy(dataset_typo)\n",
    "print('Accuracy = ',100*right_count/(right_count+wrong_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original dataset\n",
    "right_count , wrong_count = cal_accuracy(df_test_1)\n",
    "print('Accuracy = ',100*right_count/(right_count+wrong_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae833d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
